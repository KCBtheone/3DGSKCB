# =================================================================================
# =================================================================================
#               ä»3DGSæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å‡ ä½•å›¾ (æ·±åº¦/æ³•çº¿/ä¿¡åº¦/æ›²ç‡)
#
#   ç‰ˆæœ¬ï¼šV9 - å…¨è‡ªåŠ¨æ‰¹å¤„ç†ç»ˆæç‰ˆ
#   ä½œè€…ï¼šGemini AI
#
#   æœ¬è„šæœ¬æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ã€å¤šåœºæ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š
#   - æ‰¹å¤„ç†ï¼šè‡ªåŠ¨æ‰«æå¹¶å¤„ç† Mip-NeRF 360 æ•°æ®é›†ä¸‹çš„æ‰€æœ‰åœºæ™¯ã€‚
#   - æ™ºèƒ½è§£å‹ï¼šè‡ªåŠ¨æŸ¥æ‰¾å¹¶è§£å‹æ¯ä¸ªåœºæ™¯å¯¹åº”çš„3DGSæ¨¡å‹å‹ç¼©åŒ…ã€‚
#   - é«˜è´¨é‡å‡ ä½•ï¼šé›†æˆä¸‰é˜¶æ®µæ·±åº¦å›¾ä¼˜åŒ–æµç¨‹ï¼ˆå»å™ªã€æ’å€¼å¡«å……ã€åŒè¾¹æ»¤æ³¢å¹³æ»‘ï¼‰ã€‚
#   - GPUåŠ é€Ÿï¼šæ ¸å¿ƒè®¡ç®—ä½¿ç”¨PyTorchåœ¨CUDAä¸Šå®Œæˆã€‚
#   - é²æ£’æ€§ï¼šè‡ªåŠ¨å¤„ç†è·¯å¾„ã€åˆ†è¾¨ç‡å’Œå¸¸è§å™ªå£°é—®é¢˜ã€‚
#
#   ä½¿ç”¨æ–¹æ³•:
#   1. ç¡®è®¤ä¸‹æ–¹ [ 1. é…ç½®åŒºåŸŸ ] ä¸­çš„ä¸¤ä¸ªæ ¹ç›®å½•è·¯å¾„ä¸æ‚¨çš„ç¯å¢ƒå®Œå…¨åŒ¹é…ã€‚
#   2. ç¡®ä¿å·²å®‰è£…å¿…è¦çš„åº“: pip install torch numpy opencv-python plyfile tqdm scipy
#   3. åœ¨ç»ˆç«¯ä¸­ç›´æ¥è¿è¡Œ: python generate_geometry_batch.py
#
# =================================================================================
# =================================================================================

import os
import numpy as np
import cv2
from plyfile import PlyData
from tqdm import tqdm
import torch
import torch.nn.functional as F
import zipfile
from scipy.interpolate import griddata

# =================================================================================
# >>> [ 1. é…ç½®åŒºåŸŸ ] <<<
# æ‚¨åªéœ€è¦é…ç½®ä¸‹é¢è¿™ä¸¤ä¸ªæ ¹ç›®å½•ï¼
# =================================================================================

# åŒ…å«æ‰€æœ‰Mip-NeRF 360åœºæ™¯ï¼ˆå¦‚ bicycle, garden...ï¼‰çš„æ ¹ç›®å½•
NERF_360_ROOT_PATH = "/root/autodl-tmp/gaussian-splatting/data/nerf_360"

# å­˜æ”¾æ‰€æœ‰åœºæ™¯3DGSç»“æœå‹ç¼©åŒ…ï¼ˆå¦‚ kitchen.zip, garden.zip...ï¼‰çš„ç›®å½•
# æ ¹æ®æ‚¨çš„æè¿°ï¼Œå‹ç¼©åŒ…ä½äº /data ç›®å½•ä¸‹
ZIPPED_MODELS_PATH = "/root/autodl-tmp/gaussian-splatting/data"

# =================================================================================
# >>> [ 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ] <<<
# =================================================================================
def load_ply_points(ply_path, device='cuda'):
    try:
        plydata = PlyData.read(ply_path)
        points = np.vstack([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T
        return torch.from_numpy(points).to(device).float()
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°PLYæ–‡ä»¶: {ply_path}")
        return None
    except Exception as e:
        print(f"è¯»å–PLYæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def depth_to_normals(depth, fx, fy, cx, cy):
    device = depth.device
    h, w = depth.shape
    v_grid, u_grid = torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij')
    z = depth
    x = (u_grid - cx) * z / fx
    y = (v_grid - cy) * z / fy
    pts = torch.stack((x, y, z), dim=0).unsqueeze(0)
    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=device).expand(3, 1, 3, 3)
    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=device).expand(3, 1, 3, 3)
    tangent_u = F.conv2d(pts, sobel_x, padding=1, groups=3)
    tangent_v = F.conv2d(pts, sobel_y, padding=1, groups=3)
    normals = torch.cross(tangent_u.squeeze(0).permute(1, 2, 0), tangent_v.squeeze(0).permute(1, 2, 0), dim=2)
    norm = torch.linalg.norm(normals, dim=-1, keepdim=True)
    norm[norm < 1e-6] = 1e-6
    normals = normals / norm
    normals[normals[..., 2] > 0] *= -1.0
    return normals.permute(2, 0, 1)

def confidence_from_depth_gradient(depth_tensor):
    depth_np = depth_tensor.cpu().numpy()
    if depth_np.max() > 0:
        max_grad_clip = np.percentile(depth_np[depth_np > 0], 99) / 10.0
    else:
        max_grad_clip = 0.1
    sobel_x = cv2.Sobel(depth_np, cv2.CV_64F, 1, 0, ksize=3)
    sobel_y = cv2.Sobel(depth_np, cv2.CV_64F, 0, 1, ksize=3)
    grad_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
    grad_magnitude = np.clip(grad_magnitude, 0, max_grad_clip)
    max_val = grad_magnitude.max()
    uncertainty = grad_magnitude / max_val if max_val > 0 else np.zeros_like(grad_magnitude)
    confidence_map = 1.0 - uncertainty
    confidence_map[depth_np <= 0] = 0.0
    return torch.from_numpy(confidence_map).float()

def curvature_from_normals(normals_tensor):
    normals_np = normals_tensor.permute(1, 2, 0).cpu().numpy()
    laplacian_x = cv2.Laplacian(normals_np[:,:,0], cv2.CV_32F, ksize=3)
    laplacian_y = cv2.Laplacian(normals_np[:,:,1], cv2.CV_32F, ksize=3)
    laplacian_z = cv2.Laplacian(normals_np[:,:,2], cv2.CV_32F, ksize=3)
    curvature = np.sqrt(laplacian_x**2 + laplacian_y**2 + laplacian_z**2)
    if curvature.max() > 0:
        max_curv = np.percentile(curvature, 99.5)
        if max_curv > 0:
            curvature = np.clip(curvature / max_curv, 0, 1)
    return torch.from_numpy(curvature).float()

def refine_depth_map(depth_raw_tensor, median_ksize=3, interpolation_method='cubic', bilateral_d=5, bilateral_sigma=1.0):
    """(â­ æ ¸å¿ƒæ”¹è¿›) é€šè¿‡ä¸‰é˜¶æ®µæµç¨‹ä¼˜åŒ–åŸå§‹æ·±åº¦å›¾ï¼šå»å™ªã€å¡«å……å­”æ´ã€å¹³æ»‘ã€‚"""
    depth_np = depth_raw_tensor.cpu().numpy().astype(np.float32)
    h, w = depth_np.shape
    
    # é˜¶æ®µä¸€: åˆå§‹ä¸­å€¼æ»¤æ³¢å»å™ª
    depth_denoised = cv2.medianBlur(depth_np, median_ksize)
    
    # é˜¶æ®µäºŒ: é«˜è´¨é‡å­”æ´å¡«å……
    valid_mask = depth_denoised > 1e-6
    if not np.any(valid_mask): return torch.from_numpy(depth_denoised)
    
    xx, yy = np.meshgrid(np.arange(w), np.arange(h))
    known_points_coords = np.vstack([xx[valid_mask], yy[valid_mask]]).T
    known_depth_values = depth_denoised[valid_mask]
    
    filled_depth = griddata(known_points_coords, known_depth_values, (xx, yy), method=interpolation_method, fill_value=0)
    
    # é˜¶æ®µä¸‰: æœ€ç»ˆåŒè¾¹æ»¤æ³¢å¹³æ»‘
    if filled_depth.max() > 0:
        norm_depth = filled_depth / filled_depth.max()
        norm_smoothed = cv2.bilateralFilter(norm_depth.astype(np.float32), bilateral_d, bilateral_sigma, bilateral_sigma)
        final_depth_np = norm_smoothed * filled_depth.max()
    else:
        final_depth_np = filled_depth
        
    return torch.from_numpy(final_depth_np.astype(np.float32))

# =================================================================================
# >>> [ 3. ä¸»æ‰§è¡Œæµç¨‹ ] <<<
# =================================================================================

if __name__ == "__main__":
    print("åˆå§‹åŒ–å¹¶æ£€æŸ¥ç¯å¢ƒ...")
    if not torch.cuda.is_available():
        print("é”™è¯¯: æœ¬è„šæœ¬éœ€è¦CUDAæ”¯æŒçš„GPUæ‰èƒ½è¿è¡Œã€‚")
        exit()
    device = torch.device("cuda")
    print(f"ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)}")

    # è·å–æ‰€æœ‰åœºæ™¯ç›®å½•
    try:
        scene_names = [d for d in os.listdir(NERF_360_ROOT_PATH) if os.path.isdir(os.path.join(NERF_360_ROOT_PATH, d))]
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°Mip-NeRF 360æ ¹ç›®å½•: {NERF_360_ROOT_PATH}")
        print("è¯·æ£€æŸ¥ NERF_360_ROOT_PATH é…ç½®æ˜¯å¦æ­£ç¡®ã€‚")
        exit()

    print(f"å‘ç° {len(scene_names)} ä¸ªåœºæ™¯: {scene_names}")

    # --- éå†æ‰€æœ‰åœºæ™¯ ---
    for scene_name in scene_names:
        print(f"\n{'='*25} å¼€å§‹å¤„ç†åœºæ™¯: {scene_name.upper()} {'='*25}")
        
        # --- åŠ¨æ€å®šä¹‰è·¯å¾„ ---
        scene_path = os.path.join(NERF_360_ROOT_PATH, scene_name)
        model_results_path = scene_path  # æ¨¡å‹è§£å‹åå°±æ”¾åœ¨åœºæ™¯ç›®å½•é‡Œ
        rgb_image_path = os.path.join(scene_path, "images_4")
        output_base_path = os.path.join(scene_path, "derived_data")

        # --- æ™ºèƒ½è§£å‹ ---
        model_zip_path = os.path.join(ZIPPED_MODELS_PATH, f"{scene_name}.zip")
        # æ£€æŸ¥ä¸€ä¸ªå…³é”®å­ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œæ¥åˆ¤æ–­æ˜¯å¦å·²è§£å‹
        if not os.path.isdir(os.path.join(model_results_path, "checkpoint")):
            if os.path.exists(model_zip_path):
                print(f"æœªæ‰¾åˆ°è§£å‹åçš„æ¨¡å‹ï¼Œæ­£åœ¨ä» {model_zip_path} è§£å‹...")
                with zipfile.ZipFile(model_zip_path, 'r') as zip_ref:
                    zip_ref.extractall(model_results_path)
                print("è§£å‹å®Œæˆã€‚")
            else:
                print(f"è­¦å‘Š: åœºæ™¯ '{scene_name}' ç¼ºå°‘å¯¹åº”çš„æ¨¡å‹å‹ç¼©åŒ… {model_zip_path}ï¼Œè·³è¿‡æ­¤åœºæ™¯ã€‚")
                continue
        else:
            print("æ£€æµ‹åˆ°å·²è§£å‹çš„æ¨¡å‹ï¼Œè·³è¿‡è§£å‹æ­¥éª¤ã€‚")
            
        if not os.path.isdir(rgb_image_path):
            print(f"è­¦å‘Š: åœ¨åœºæ™¯ '{scene_name}' ä¸­æ‰¾ä¸åˆ°RGBå›¾åƒæ–‡ä»¶å¤¹: {rgb_image_path}ï¼Œè·³è¿‡æ­¤åœºæ™¯ã€‚")
            continue

        # --- åŠ è½½3DGSæ¨¡å‹ (æ¯ä¸ªåœºæ™¯åŠ è½½ä¸€æ¬¡) ---
        ply_file_path = os.path.join(model_results_path, "checkpoint/point_cloud/iteration_30000/point_cloud.ply")
        print(f"æ­£åœ¨ä» {ply_file_path} åŠ è½½é«˜æ–¯ç‚¹äº‘åˆ°GPU...")
        points_world = load_ply_points(ply_file_path, device=device)
        if points_world is None:
            print(f"é”™è¯¯: åŠ è½½ç‚¹äº‘å¤±è´¥ï¼Œè·³è¿‡åœºæ™¯ '{scene_name}'ã€‚")
            continue
            
        ones = torch.ones((points_world.shape[0], 1), device=device)
        points_world_homo = torch.cat([points_world, ones], dim=1)
        print(f"æˆåŠŸåŠ è½½ {points_world.shape[0]} ä¸ªç‚¹ã€‚")

        # --- å‡†å¤‡è¾“å‡ºç›®å½• ---
        os.makedirs(output_base_path, exist_ok=True)
        depth_out_dir = os.path.join(output_base_path, "depth")
        normal_out_dir = os.path.join(output_base_path, "normal")
        confidence_out_dir = os.path.join(output_base_path, "confidence")
        curvature_out_dir = os.path.join(output_base_path, "curvature")
        os.makedirs(depth_out_dir, exist_ok=True)
        os.makedirs(normal_out_dir, exist_ok=True)
        os.makedirs(confidence_out_dir, exist_ok=True)
        os.makedirs(curvature_out_dir, exist_ok=True)
    
        # --- éå†æ‰€æœ‰ç›¸æœº ---
        cameras_path = os.path.join(model_results_path, "predictions/cameras")
        if not os.path.isdir(cameras_path):
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°ç›¸æœºå‚æ•°æ–‡ä»¶å¤¹: {cameras_path}ï¼Œè·³è¿‡åœºæ™¯ '{scene_name}'ã€‚")
            continue
        camera_files = sorted([f for f in os.listdir(cameras_path) if f.endswith('.npz')])
        
        print(f"æ‰¾åˆ° {len(camera_files)} ä¸ªç›¸æœºè§†è§’ï¼Œå¼€å§‹å¤„ç†...")
        for cam_file in tqdm(camera_files, desc=f"å¤„ç† {scene_name} è§†è§’"):
            cam_name = os.path.splitext(cam_file)[0]
            
            # æ­¥éª¤ 1: åŠ¨æ€è¯»å–å›¾åƒåˆ†è¾¨ç‡
            possible_extensions = ['.png', '.PNG', '.jpg', '.JPG', '.jpeg', '.JPEG']
            rgb_file_path = next((os.path.join(rgb_image_path, f"{cam_name}{ext}") for ext in possible_extensions if os.path.exists(os.path.join(rgb_image_path, f"{cam_name}{ext}"))), None)
            if rgb_file_path is None:
                tqdm.write(f"è­¦å‘Š: åœ¨ '{rgb_image_path}' ä¸­æ‰¾ä¸åˆ°RGBå›¾åƒ {cam_name}ï¼Œè·³è¿‡ã€‚")
                continue
            h, w, _ = cv2.imread(rgb_file_path).shape
            
            # æ­¥éª¤ 2: è·å–ç›¸æœºå‚æ•°
            cam_data = np.load(os.path.join(cameras_path, cam_file))
            try:
                intrinsics_data = torch.from_numpy(cam_data['intrinsics']).to(device).float()
                c2w_3x4 = torch.from_numpy(cam_data['poses']).to(device).float()
            except KeyError as e:
                tqdm.write(f"é”™è¯¯: ç›¸æœºæ–‡ä»¶ {cam_file} ç¼ºå°‘Key: {e}ï¼Œè·³è¿‡ã€‚")
                continue

            fx, fy, cx, cy = intrinsics_data[0], intrinsics_data[1], intrinsics_data[2], intrinsics_data[3]
            c2w_4x4 = torch.cat([c2w_3x4, torch.tensor([[0.0, 0.0, 0.0, 1.0]], device=device)], dim=0)
            w2c = torch.inverse(c2w_4x4)
            
            # æ­¥éª¤ 3: æŠ•å½±ç‚¹äº‘ç”ŸæˆåŸå§‹æ·±åº¦å›¾
            points_camera = (w2c @ points_world_homo.T).T
            points_camera_xyz = points_camera[:, :3]
            depths = points_camera_xyz[:, 2]
            u = (points_camera_xyz[:, 0] * fx / depths) + cx
            v = (points_camera_xyz[:, 1] * fy / depths) + cy
            mask = (depths > 1e-3) & (u >= 0) & (u < w) & (v >= 0) & (v < h)
            u_idx, v_idx = u[mask].long(), v[mask].long()
            flat_indices = v_idx * w + u_idx
            flat_depth = torch.full((h * w,), float('inf'), device=device)
            flat_depth.scatter_reduce_(0, flat_indices, depths[mask], reduce='amin', include_self=False)
            depth_map_raw = flat_depth.view(h, w)
            depth_map_raw[depth_map_raw == float('inf')] = 0
            
            # æ­¥éª¤ 4: (â­æ ¸å¿ƒæ”¹è¿›â­) ä½¿ç”¨é«˜çº§ä¸‰é˜¶æ®µæµç¨‹ä¼˜åŒ–æ·±åº¦å›¾
            depth_map = refine_depth_map(depth_map_raw, interpolation_method='cubic').to(device)
            
            # æ­¥éª¤ 5: è®¡ç®—æ‰€æœ‰è¡ç”Ÿå‡ ä½•å›¾
            with torch.no_grad():
                normals = depth_to_normals(depth_map, fx, fy, cx, cy)
                confidence = confidence_from_depth_gradient(depth_map)
                curvature = curvature_from_normals(normals)
            
            # æ­¥éª¤ 6: ä¿å­˜ç»“æœ
            cv2.imwrite(os.path.join(depth_out_dir, f"{cam_name}.png"), (depth_map.cpu().numpy() * 1000).astype(np.uint16))
            cv2.imwrite(os.path.join(normal_out_dir, f"{cam_name}.png"), cv2.cvtColor(((normals.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0 * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))
            cv2.imwrite(os.path.join(confidence_out_dir, f"{cam_name}.png"), (confidence.numpy() * 255).astype(np.uint8))
            cv2.imwrite(os.path.join(curvature_out_dir, f"{cam_name}.png"), (curvature.numpy() * 255).astype(np.uint8))

        print(f"[âœ”] åœºæ™¯ {scene_name.upper()} å¤„ç†å®Œæ¯•ï¼ç»“æœä¿å­˜åœ¨: {output_base_path}")

    print("\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰åœºæ™¯å‡å·²å¤„ç†å®Œæ¯•ï¼ğŸ‰ğŸ‰ğŸ‰")