import cv2
import numpy as np
import os
import json
import argparse
from tqdm import tqdm
import sys

# [æ ¸å¿ƒä¿®æ”¹ 1/3] å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ colmap_loader
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from scene.colmap_loader import read_extrinsics_binary, read_extrinsics_text
except ImportError:
    print("âŒ é”™è¯¯: æ— æ³•ä» 'scene.colmap_loader' å¯¼å…¥å‡½æ•°ã€‚")
    print("   è¯·ç¡®ä¿æ‚¨æ˜¯ä» gaussian-splatting é¡¹ç›®çš„æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬ã€‚")
    sys.exit(1)


# [æ ¸å¿ƒæ–°å¢] æ–°å¢å‡½æ•°ï¼Œç”¨äºä» images.bin æ–‡ä»¶ä¸­è¯»å–å›¾åƒå
def read_colmap_image_names_from_bin(images_bin_path):
    """
    ä» COLMAP çš„ images.bin æ–‡ä»¶ä¸­è§£æå‡ºæ‰€æœ‰å›¾åƒçš„æ–‡ä»¶åã€‚
    """
    extrinsics = read_extrinsics_binary(images_bin_path)
    # extrinsics æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå€¼ä¸º CameraInfo å¯¹è±¡ï¼Œå…¶ .name å±æ€§å°±æ˜¯æ–‡ä»¶å
    image_names = [ext.name for ext in extrinsics.values()]
    return sorted(list(set(image_names)))

def read_colmap_image_names_from_txt(images_txt_path):
    """
    (åŸå§‹å‡½æ•°) ä» COLMAP çš„ images.txt æ–‡ä»¶ä¸­è§£æå‡ºæ‰€æœ‰å›¾åƒçš„æ–‡ä»¶åã€‚
    """
    image_names = []
    with open(images_txt_path, "r") as f:
        lines = f.readlines()
    for i in range(len(lines)):
        line = lines[i].strip()
        if line.startswith("#"): continue
        # åœ¨ images.txt ä¸­ï¼Œæ¯ä¸¤è¡Œæè¿°ä¸€ä¸ªå›¾åƒï¼Œç¬¬äºŒè¡ŒåŒ…å«æ–‡ä»¶å
        if (i - 4) % 2 == 0 and i >= 4:
            parts = line.split()
            if len(parts) > 9:
                image_names.append(parts[-1])
    return sorted(list(set(image_names)))

def detect_lines_in_scene(dataset_path, image_dir="images", visualize=False, 
                          detector='lsd', 
                          min_filter_length=40,
                          hough_threshold=150, min_length=50, max_gap=10):
    """
    å¯¹æŒ‡å®šåœºæ™¯ä¸­çš„æ‰€æœ‰å›¾åƒè¿è¡Œç›´çº¿æ£€æµ‹ï¼Œå¹¶ç”Ÿæˆ lines.json æ–‡ä»¶ã€‚
    ç°åœ¨å¯ä»¥è‡ªåŠ¨æ£€æµ‹å¹¶è¯»å– images.bin æˆ– images.txtã€‚
    """
    sparse_dir = os.path.join(dataset_path, "sparse", "0")
    images_bin_path = os.path.join(sparse_dir, "images.bin")
    images_txt_path = os.path.join(sparse_dir, "images.txt")
    image_folder = os.path.join(dataset_path, image_dir)
    image_names = []

    # [æ ¸å¿ƒä¿®æ”¹ 2/3] æ™ºèƒ½åˆ¤æ–­åº”è¯¥è¯»å– .bin è¿˜æ˜¯ .txt æ–‡ä»¶
    if os.path.exists(images_bin_path):
        print(f"âœ… æ‰¾åˆ°äºŒè¿›åˆ¶ COLMAP æ–‡ä»¶: '{images_bin_path}'")
        try:
            image_names = read_colmap_image_names_from_bin(images_bin_path)
        except Exception as e:
            print(f"âŒ é”™è¯¯: è§£æ '{images_bin_path}' å¤±è´¥: {e}")
            return
    elif os.path.exists(images_txt_path):
        print(f"âœ… æ‰¾åˆ°æ–‡æœ¬ COLMAP æ–‡ä»¶: '{images_txt_path}'")
        try:
            image_names = read_colmap_image_names_from_txt(images_txt_path)
        except Exception as e:
            print(f"âŒ é”™è¯¯: è§£æ '{images_txt_path}' å¤±è´¥: {e}")
            return
    else:
        print(f"âŒ é”™è¯¯: åœ¨ '{sparse_dir}' ç›®å½•ä¸‹æ—¢æœªæ‰¾åˆ° 'images.bin' ä¹Ÿæœªæ‰¾åˆ° 'images.txt'ã€‚")
        return

    image_paths = [os.path.join(image_folder, name) for name in image_names]
    print(f"ğŸ” ä» COLMAP å…ƒæ•°æ®ä¸­æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒã€‚ä½¿ç”¨ [{detector.upper()}] æ£€æµ‹å™¨å¼€å§‹å¤„ç†...")
    
    if detector == 'lsd':
        print(f"ğŸ“ å°†è¿‡æ»¤æ‰æ‰€æœ‰é•¿åº¦å°äº {min_filter_length} åƒç´ çš„ç›´çº¿ã€‚")

    if visualize:
        vis_dir = os.path.join(dataset_path, "lines_visualization")
        os.makedirs(vis_dir, exist_ok=True)
        print(f"ğŸ–¼ï¸ å¯è§†åŒ–ç»“æœå°†ä¿å­˜è‡³: '{vis_dir}'")

    line_detector = None
    if detector == 'lsd':
        line_detector = cv2.createLineSegmentDetector(0)
            
    all_lines_data = {}
    for image_path in tqdm(image_paths, desc=f"Processing {os.path.basename(dataset_path)}"):
        image_basename = os.path.basename(image_path)
        if not os.path.exists(image_path):
            all_lines_data[image_basename] = []
            continue

        try:
            image = cv2.imread(image_path)
            if image is None:
                all_lines_data[image_basename] = []
                continue
            
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            lines = None
            if detector == 'lsd':
                lines, _, _, _ = line_detector.detect(gray)
            elif detector == 'hough':
                lines = cv2.HoughLinesP(gray, 1, np.pi / 180, threshold=hough_threshold,
                                        minLineLength=min_length, maxLineGap=max_gap)
            
            filtered_lines = []
            if lines is not None:
                for line in lines:
                    x1, y1, x2, y2 = line[0]
                    length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
                    if length >= min_filter_length:
                        filtered_lines.append([[int(x1), int(y1)], [int(x2), int(y2)]])
            
            all_lines_data[image_basename] = filtered_lines

            if visualize and filtered_lines:
                vis_image = image.copy()
                for line_coords in filtered_lines:
                    pt1 = tuple(line_coords[0])
                    pt2 = tuple(line_coords[1])
                    cv2.line(vis_image, pt1, pt2, (0, 255, 0), 2)
                cv2.imwrite(os.path.join(vis_dir, image_basename), vis_image)

        except Exception as e:
            print(f"\nå¤„ç† '{image_path}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            all_lines_data[image_basename] = []

    output_path = os.path.join(dataset_path, "lines.json")
    try:
        with open(output_path, 'w') as f:
            json.dump(all_lines_data, f, indent=4)
        print(f"âœ… ç›´çº¿æ£€æµ‹å®Œæˆã€‚ç»“æœå·²ä¿å­˜è‡³: '{output_path}'")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•ä¿å­˜jsonæ–‡ä»¶è‡³ '{output_path}'. åŸå› : {e}")

if __name__ == "__main__":
    # [æ ¸å¿ƒä¿®æ”¹ 3/3] ç¡®ä¿ argparse çš„ description æ›´æ¸…æ™°
    parser = argparse.ArgumentParser(description="åœ¨ COLMAP åœºæ™¯ä¸­æ£€æµ‹ç›´çº¿ï¼Œæ”¯æŒ .bin å’Œ .txt æ ¼å¼ã€‚")
    parser.add_argument("--dataset_path", type=str, required=True, help="æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„ (ä¾‹å¦‚: .../nerf_360/bicycle)")
    parser.add_argument("--image_dir", type=str, default="images", help="å­˜æ”¾å›¾åƒçš„å­ç›®å½•å (ä¾‹å¦‚: images_4)")
    parser.add_argument("--detector", type=str, default='lsd', choices=['lsd', 'hough'], help="é€‰æ‹©ä½¿ç”¨çš„ç›´çº¿æ£€æµ‹ç®—æ³•")
    parser.add_argument("--min_filter_length", type=int, default=40, help="åå¤„ç†è¿‡æ»¤å™¨ï¼šä¿ç•™çš„æœ€å°ç›´çº¿é•¿åº¦")
    parser.add_argument("--visualize", action='store_true', help="æ˜¯å¦ç”Ÿæˆå¹¶ä¿å­˜å¸¦æœ‰ç›´çº¿çš„å¯è§†åŒ–å›¾åƒ")
    
    # éœå¤«å˜æ¢ä¸“ç”¨å‚æ•°
    parser.add_argument("--hough_threshold", type=int, default=150, help="[ä»…Hough] éœå¤«å˜æ¢çš„é˜ˆå€¼")
    parser.add_argument("--min_length", type=int, default=50, help="[ä»…Hough] éœå¤«å˜æ¢æ£€æµ‹çš„æœ€å°çº¿æ®µé•¿åº¦")
    parser.add_argument("--max_gap", type=int, default=10, help="[ä»…Hough] éœå¤«å˜æ¢å…è®¸çš„æœ€å¤§çº¿æ®µé—´éš™")
    
    args = parser.parse_args()
    
    detect_lines_in_scene(args.dataset_path, args.image_dir, args.visualize,
                          args.detector, args.min_filter_length,
                          args.hough_threshold, args.min_length, args.max_gap)